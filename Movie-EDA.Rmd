---
title: "Movie Revenue"
output: 
  html_document:
    toc: TRUE
    theme: cerulean
    number_sections: TRUE
    code_folding: show
---

```{r, message = FALSE}
library(tidyverse)
library(caret)
library(Amelia)
library(stringr)
library(DataExplorer)
library(fastDummies)
library(forcats)
library(purrr)
library(lubridate)
library(scales)
library(patchwork)
```

# Exploratory Data Analysis

```{r, warning=FALSE}
movie_train <- read.csv("train.csv", stringsAsFactors = FALSE, na.strings = c("", NA))
movie_test <- read.csv("test.csv", stringsAsFactors = FALSE, na.strings = c("", NA))
movie_data <- bind_rows(movie_train, movie_test)

colnames(movie_data)[1] <- "id"
colnames(movie_train)[1] <- "id"
colnames(movie_test)[1] <- "id"

glimpse(movie_data)
```

## Exploratory Plots

When looking at the plot that shows the proportion of missing values for each variable in the dataset. There appears to be a lot of missing values in a few of the variables and then a number of the variables have only a few missing values. Although it shows that there are a lot of missing values for revenue, that isn't a problem because those are missing values from the test set since the revenue is not included in the test set. For dealing with the missing values, it is easier to combine the training and test set and then to split them apart again after deciding what to do with the missing values. 

```{r, message=FALSE, warning=FALSE}
# Plot of how many values are missing
plot_missing(movie_data)
```

There are only a few numeric variables, so the relationship between those variables and revenue are explored in the scatterplots below. 
```{r, warning=FALSE, message=FALSE}
# Plots of numeric variables
p1 <- ggplot(movie_data, aes(x = popularity, y = revenue)) + 
  geom_point() + 
  geom_smooth(method = "loess", se = FALSE) + 
  xlim(0, 300) + 
  labs(x = "Popularity", y = "Revenue") + 
  scale_y_continuous(labels = comma) +
  theme_classic()
p2 <- ggplot(movie_data, aes(x = runtime, y = revenue)) + 
  geom_point() + 
  geom_smooth(method = "loess", se = FALSE) +
  labs(x = "Runtime (minutes)", y = "Revenue") +
  scale_y_continuous(labels = comma) +
  theme_classic()

p3 <- ggplot(movie_data, aes(x = budget, y = revenue)) +
  geom_point() + 
  geom_smooth(method = "loess", se = FALSE) +
  labs(x = "Budget", y = "Revenue") + 
  scale_y_continuous(labels = comma) +
  scale_x_continuous(labels = comma) +
  theme_classic()

(p1 + p2)/p3
```

With most of the categorical variables, there is data manipulation and cleaning that needs to be done to extract the useful information. Most of the important variables are in JSON format and the relevant information will be extracted using regular expressions mostly. 

# Data Cleaning

## Initial Variable Dropping

There are some variables, `imdb_id`, `original_title`, `overview`, `poster_path`, and `status` that don't provide a lot of information about the revenue a movie brings in. As a result, those variables are dropped from the dataset. There is also one movie that did not have a release date listed, so I looked up the release data for the movie. After adding that release date, the release date was changed from a character to a date variable. 

```{r}
movie_data <- movie_data %>% select(-imdb_id, -original_title, -overview, -poster_path, -status)

# Insert the release data for the one missing value
movie_data[is.na(movie_data$release_date),]$release_date <- "05/01/00"
# Change release date to a date
movie_data$release_date <- parse_date_time2(movie_data$release_date, orders = "%m%d%y", cutoff_2000 = 20L)
```

## Categorical variables

Next, I want to look at the variables that have a high percentage of missing values. The variables `belongs_to_collection` and `homepage` are missing more than 50% of the time. `Keywords` and `tagline` are the other variables that have quite a few missing values. The revenue variable is not of concern to me here since the missing values for revenue come from the test set. Since there are so many missing values for these variables, I created dummy variables to indicate whether or not an observation had keywords, a tagline, a homepage, or belonged to a collection. The boxplots below show movies that have keywords, a tagline, a homepage, or belong to collection have a higher revenue on average. Even though the difference is small for `homepage`, I will drop the actual variables and use the dummy variables for all four of the variables. Also, the log of revenue is used to better show the difference between the group and to limit the influence outliers have on the plots. 

```{r}
# Analyzing features with many NAs

movie_nas <- movie_train %>%
  select(Keywords, tagline, homepage, belongs_to_collection, revenue) %>%
  mutate_at(.funs = list(cat = ~ifelse(is.na(.), 0, 1)), 
            .vars = c("Keywords", "tagline", "homepage",
                      "belongs_to_collection"))
head(movie_nas)

ggplot(movie_nas, aes(x = factor(belongs_to_collection_cat), y = log(revenue))) + 
  geom_jitter(alpha = .2, color = 'steelblue') +
  geom_boxplot(alpha = .1) +
  scale_x_discrete(labels = c("No", "Yes")) +
  labs(x = 'Belongs to a Collection', y = 'Log(Revenue)')

ggplot(movie_nas, aes(x = factor(Keywords_cat), y = log(revenue))) + 
  geom_jitter(alpha = .2, color = 'steelblue') +
  geom_boxplot(alpha = .1) +
  scale_x_discrete(labels = c("No", "Yes")) +
  labs(x = 'Keywords', y = 'Log(Revenue)')

ggplot(movie_nas, aes(x = factor(homepage_cat), y = log(revenue))) + 
  geom_jitter(alpha = .2, color = 'steelblue') +
  geom_boxplot(alpha = .1) +
  scale_x_discrete(labels = c("No", "Yes")) +
  labs(x = 'Homepage', y = 'Log(Revenue)')

ggplot(movie_nas, aes(x = factor(tagline_cat), y = log(revenue))) + 
  geom_jitter(alpha = .2, color = 'steelblue') +
  geom_boxplot(alpha = .1) +
  scale_x_discrete(labels = c("No", "Yes")) +
  labs(x = 'Homepage', y = 'Log(Revenue)')

# Add indicator columns to main data
movie_data <- movie_data %>% 
  mutate_at(.funs = list(ind = ~ifelse(is.na(.), 0, 1)),
            .vars = c("Keywords", "tagline", "homepage",
                      "belongs_to_collection"))

# Drop columns
movie_data <- movie_data %>%
  select(-belongs_to_collection, -homepage, -tagline, -Keywords)
```

## Cast and Crew (counts for each)

There were a number of ideas of how to work with the cast and crew. There is a possibility of pulling out the most commonly known actors and directors to see if those movies generally had a higher revenue. I also thought about counting the number of high profile stars in each movie and seeing if that has an influence on the revenue. In the end, I simply decided to count the number of crew members and the number of cast members with the logic that movies that have a higher budget probably have more cast and crew members. From the earlier plots, budget has a positive correlation with revenue so it is likely that there is more cast and crew members, that the revenue is also higher (If this isn't a case, then a lot of money is being lost paying additional cast and crew members that don't help bring in revenue). There are a few missing values for crew variable, so the median number of crew members was imputed for the missing values. I think it may be more useful to fit a model that uses the budget and a few of the other variables to determine the number of crew members, but since there are only a few movies that have the crew information missing, I will use the median for those. 



```{r}
movie_data$cast_count <- str_count(movie_data$cast, "'name':\\s'")
movie_data$crew_count <- str_count(movie_data$crew, "'name':\\s'")

# Impute median for missing crew_count values
movie_data[is.na(movie_data$crew_count),]$crew_count <- median(movie_data$crew_count, na.rm=TRUE)

p8 <- ggplot(movie_data, aes(x = cast_count)) + 
  geom_density(fill= "coral3") + 
  labs(x = "Number of Cast Members", y = "Density")

p9 <- ggplot(movie_data, aes(x = crew_count)) +
  geom_density(fill = "coral3") +
  labs(x = "Number of Crew Members", y = "Density")


p10 <- ggplot(movie_data, aes(x = cast_count, y = revenue)) +
  geom_point(color = "coral3") + 
  scale_y_continuous(labels = comma) +
  labs(x = "Number of CastMembers", y = "Revenue")

p11 <- ggplot(movie_data, aes(x = crew_count, y = revenue)) +
  geom_point(color = "coral3") + 
  scale_y_continuous(labels = comma) +
  labs(x = "Number of Crew Members", y = "Revenue")

(p8 + p9)/(p10 + p11)
```

## Original Language

```{r}
# Most of the movies are English, so I'm going to add another indicator variable to specify if the original language was English or not
movie_data <- movie_data %>% mutate(English = ifelse(original_language == "en", 1, 0))

# Drop original langauge column
movie_data <- movie_data %>% select(-original_language)

```

## Function to extract useful data

```{r}
extract_n_most_freq_to_dummy <- function(vect, pattern, name, n_most_pattern_fun, n_most_freq=NULL) {
  
  # Get all vars into a usable list
  var_extractor <- function(x) {
      unlist(str_extract_all(x, pattern=pattern))
  }
  
  if(!is.null(n_most_freq)) {
    n_most_freq <- n_most_freq
    extracted_vars <- lapply(vect, var_extractor) %>%
      unlist()
    
    # Find n most frequently appearing vars
    var_freq <- tibble(var = extracted_vars) %>%
      group_by(var) %>%
      count() %>%
      arrange(desc(n)) %>%
      ungroup() %>%
      filter(!is.na(var)) %>%
      top_n(n_most_freq, n)
    
    # Create new regular expression for most frequently occuring vars
    var_regex <- n_most_pattern_fun(var_freq$var)
    # var_regex <- paste(var_freq$var, collapse="|") %>%
    #   str_replace_all('([^a-zA-Z0-9 |\\-])', '\\\\\\1')
    
    new_vars <- sapply(vect, function(x) {
      unlist(str_extract_all(x, var_regex))
      })
    
    dummy_vars <- qdapTools::mtabulate(new_vars) 
  }
  else {
    extracted_vars <- sapply(vect, var_extractor)
    dummy_vars <- qdapTools::mtabulate(extracted_vars)
  }
  
  colnames(dummy_vars) <- paste0(name, gsub("\\W", "", colnames(dummy_vars)))
  rownames(dummy_vars) <- 1:nrow(dummy_vars)
  
  return(dummy_vars)
}
```

### Spoken Languages

```{r}
pattern <- "(?<=')[:lower:]{2}(?=')"
pattern_fun <- function(x) {
  paste(x, collapse="|") %>%
    paste0("(?<=')(", ., ")")
}
name <- 'Sp_'
n <- 7

dummy_sp <- extract_n_most_freq_to_dummy(movie_data$spoken_languages, pattern, name, pattern_fun, n)

movie_data <- cbind(movie_data, dummy_sp)

# Number of spoken languages
movie_data$Num_Languages <- movie_data %>%
  select(Sp_de:Sp_ru) %>%
  rowSums()
```

### Genres

```{r}
pattern <- "(?<=\')([A-Z])\\w+(.*?)(?=\')"
pattern_fun <- function(x) {
  paste(x, collapse="|")
}
name <- 'Genre'
n <- NULL

dummy_g <- extract_n_most_freq_to_dummy(movie_data$genres, pattern, name, pattern_fun, n)

movie_data <- cbind(movie_data, dummy_g)

# number of genres
movie_data$num_genres <- movie_data %>% select(GenreAction:GenreWestern) %>% rowSums()
```

### Countries

```{r country}
pattern <- "(?<=name': ')[A-Za-z\\s]+"
pattern_fun <- function(x) {
  paste(x, collapse="|")
}
name <- 'Country'
n <- 10

dummy_c <- extract_n_most_freq_to_dummy(movie_data$production_countries, pattern, name, pattern_fun, n)

movie_data <- cbind(movie_data, dummy_c)
```

### Production Companies

```{r}
pattern <- "(?<=name': ')[^']+"
pattern_fun <- function(x) {
  paste(x, collapse="|") %>%
      str_replace_all('([^a-zA-Z0-9 |\\-])', '\\\\\\1')
}
name <- 'Comp'
n <- 30

dummy_comp <- extract_n_most_freq_to_dummy(movie_data$production_companies, pattern, name, pattern_fun, n)

movie_data <- cbind(movie_data, dummy_comp)
```

## Imputation: Runtime

```{r}
# Median imputation for the runtime variable
movie_data[is.na(movie_data$runtime),]$runtime <- median(movie_data[!is.na(movie_data$runtime),]$runtime)
```

## Remove original string variables
```{r}
movie_data_1 <- movie_data %>%
  select(-c(id, genres, production_companies,
            production_countries, spoken_languages, 
            title, cast, crew))
```

# Preprocessing

```{r}
# Split dataset into original train/test split
movie.train.pre <- movie_data_1 %>%
  filter(!is.na(revenue)) %>%
  select(-revenue)
movie.test.pre <- movie_data_1 %>%
  filter(is.na(revenue)) %>% 
  select(-revenue)
```

## PCA Dataset

```{r}
# Remove variables with near-zero variance and reduce dimensionality
train <- preProcess(movie.train.pre, method = c("nzv", "pca"))
movie.train <- predict(train, movie.train.pre)
movie.test <- predict(train, movie.test.pre)

movie.train <- movie.train %>% 
  mutate(revenue = movie_data_1$revenue[!is.na(movie_data_1$revenue)])
```

## No PCA Dataset

```{r}
# Remove variables with near-zero variance and reduce dimensionality
train <- preProcess(movie.train.pre, method = c("nzv", "center", "scale"))
movie.train.nopca <- predict(train, movie.train.pre)
movie.test.nopca <- predict(train, movie.test.pre)

movie.train.nopca <- movie.train.nopca %>% 
  mutate(revenue = movie_data_1$revenue[!is.na(movie_data_1$revenue)])
```

# Predictions

## Prepping for caret library

```{r}
# Using rmsle as scoring function for cross validation fitting
custom_summary <- function(data, lev = NULL, model = NULL) {
  out <- Metrics::rmsle(data[, "obs"], data[, "pred"])
  names(out) <- c("rmsle")
  return(out)
}

myControl <- trainControl(method = "cv",
                          number = 10,
                          summaryFunction = custom_summary)

# Set random seed
set.seed(1989)
```

## Log Random Forest

```{r}
model <- train(log(revenue)~.,
                  data = movie.train,
                  method = "ranger",
                  tuneLength = 5,
                  trControl = myControl,
                  metric = "rmsle",
                  maximize = FALSE); beepr::beep(3)
model

preds <- predict(model, newdata = movie.test) %>% exp()
dat <- data.frame(id = movie_test$id, revenue = preds)
write_csv(dat, "log-rf-preds.csv")
```

## Glmnet

```{r}
model2 <- train(log(revenue)~.,
                data = movie.train,
                method = "glmnet",
                tuneLength = 10,
                trControl = myControl,
                metric ="rmsle",
                maximize = FALSE); beepr::beep(3)
model2

preds <- predict(model2, newdata = movie.test) %>% exp()
dat <- data.frame(id = movie_test$id, revenue = preds)
write_csv(dat, "glmnet-preds.csv")
```

## Bagged MARS

```{r}
model3 <- train(log(revenue)~.,
                data = movie.train,
                method = "bagEarth",
                tuneLength = 5,
                trControl = myControl,
                metric ="rmsle",
                maximize = FALSE); beepr::beep(3)
model3

preds <- predict(model3, newdata = movie.test) %>% exp()
dat <- data.frame(id = movie_test$id, revenue = preds)
write_csv(dat, "mars-preds.csv")
```

## SVM

```{r}
model4 <- train(log(revenue)~.,
                data = movie.train,
                method = "svmRadial",
                tuneLength = 5,
                trControl = myControl,
                metric ="rmsle",
                maximize = FALSE); beepr::beep(3)
model4

preds <- predict(model4, newdata = movie.test) %>% exp()
dat <- data.frame(id = movie_test$id, revenue = preds)
write_csv(dat, "svm-preds.csv")
```

## Random Forest, No PCA (2.15475 score)

```{r}

grid <- expand.grid("mtry" = c(20, 25, 30), "splitrule" = "extratrees", min.node.size = c(1, 3, 5))

model.rf2 <- train(log(revenue)~.,
                  data = movie.train.nopca,
                  method = "ranger",
                  tuneGrid = grid,
                  trControl = myControl,
                  metric = "rmsle",
                  maximize = FALSE); beepr::beep(3)

model.rf2

preds.rf2 <- predict(model.rf2, newdata = movie.test.nopca) %>% exp()
rf.dat2 <- data.frame(id = movie_test[,1], revenue = preds.rf2)
write_csv(rf.dat2, path = "rf_nopca2.csv")

```

## Gradient Boosting Forest, No PCA (2.07568 score)

```{r}
grid <- expand.grid("n.trees"=500, "interaction.depth"=4, "shrinkage"=0.1, "n.minobsinnode"=20)

rfb.mod <- train(log(revenue)~.,
                 data=movie.train.nopca,
                 method="gbm",
                 tuneGrid=grid,
                 trControl=myControl,
                 metric = "rmsle",
                 verbose=F); beepr::beep(3)

preds <- predict(rfb.mod, newdata = movie.test.nopca) %>% exp()
dat <- data.frame(id = movie_test$id, revenue = preds)
write_csv(dat, "rf-gboost-preds.csv")
```

## xgbLinear, No PCA (2.1869 score)

```{r}
myControl <- trainControl(method = "cv",
                          number = 3,
                          summaryFunction = custom_summary)

tunegrid <- expand.grid(nrounds = 60,
                        lambda = .01,
                        alpha = .01,
                        eta = .2)

xgbLinear.model <- train(log(revenue)~.,
                   data = movie.train.nopca,
                   method = "xgbLinear",
                   tuneGrid = tunegrid,
                   trControl = myControl,
                   metric = "rmsle",
                   maximize = FALSE); beepr::beep(3)
xgbLinear.model

preds <- predict(xgbLinear.model, newdata = movie.test.nopca) %>% exp()
dat <- data.frame(id = movie_test$id, revenue = preds)
write_csv(dat, "xgbLinear-preds.csv")
```

## XGB Tree (2.09998 score)

```{r}
tunegrid <- expand.grid(eta = .25,
                        max_depth = 3,
                        colsample_bytree = .9,
                        subsample = .8,
                        nrounds = 100,
                        min_child_weight = 1,
                        gamma = .075)

xgbTree.model <- train(log(revenue)~.,
                   data = movie.train,
                   method = "xgbTree",
                   tuneGrid = tunegrid,
                   trControl = myControl,
                   metric = "rmsle",
                   maximize = FALSE
)
xgbTree.model

preds <- predict(xgbTree.model, newdata = movie.test) %>% exp()
dat <- data.frame(id = movie_test$id, revenue = preds)
write_csv(dat, "xgbTree-preds.csv")
```
